"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[9419],{1985:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"ai":[{"type":"link","label":"Overview","href":"/ai-on-eks/docs/ai/","docId":"ai/index","unlisted":false},{"type":"link","label":"Infrastructure","href":"/ai-on-eks/docs/ai/infrastructure/","docId":"ai/infrastructure/infrastructure","unlisted":false},{"type":"category","label":"Training on EKS","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"GPUs","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"BioNeMo on EKS","href":"/ai-on-eks/docs/ai/training/GPUs/bionemo","docId":"ai/training/GPUs/bionemo","unlisted":false}]},{"type":"category","label":"Neuron","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Llama-2 with RayTrain on Trn1","href":"/ai-on-eks/docs/ai/training/Neuron/RayTrain-Llama2","docId":"ai/training/Neuron/RayTrain-Llama2","unlisted":false},{"type":"link","label":"Llama-2 with Nemo-Megatron on Trn1","href":"/ai-on-eks/docs/ai/training/Neuron/Llama2","docId":"ai/training/Neuron/Llama2","unlisted":false},{"type":"link","label":"BERT-Large on Trainium","href":"/ai-on-eks/docs/ai/training/Neuron/BERT-Large","docId":"ai/training/Neuron/BERT-Large","unlisted":false}]}],"href":"/ai-on-eks/docs/category/training-on-eks"},{"type":"category","label":"Inference on EKS","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"GPUs","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"RayServe with vLLM","href":"/ai-on-eks/docs/ai/inference/GPUs/vLLM-rayserve","docId":"ai/inference/GPUs/vLLM-rayserve","unlisted":false},{"type":"link","label":"NVIDIA Triton Server with vLLM","href":"/ai-on-eks/docs/ai/inference/GPUs/vLLM-NVIDIATritonServer","docId":"ai/inference/GPUs/vLLM-NVIDIATritonServer","unlisted":false},{"type":"link","label":"Stable Diffusion on GPU","href":"/ai-on-eks/docs/ai/inference/GPUs/stablediffusion-gpus","docId":"ai/inference/GPUs/stablediffusion-gpus","unlisted":false},{"type":"link","label":"NVIDIA NIM LLM on Amazon EKS","href":"/ai-on-eks/docs/ai/inference/GPUs/nvidia-nim-llama3","docId":"ai/inference/GPUs/nvidia-nim-llama3","unlisted":false},{"type":"link","label":"NVIDIA NIM Operator on EKS","href":"/ai-on-eks/docs/ai/inference/GPUs/nvidia-nim-operator","docId":"ai/inference/GPUs/nvidia-nim-operator","unlisted":false},{"type":"link","label":"DeepSeek-R1 on EKS","href":"/ai-on-eks/docs/ai/inference/GPUs/ray-vllm-deepseek","docId":"ai/inference/GPUs/ray-vllm-deepseek","unlisted":false}]},{"type":"category","label":"Neuron","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Llama-3-8B with vLLM on Inferentia2","href":"/ai-on-eks/docs/ai/inference/Neuron/vllm-ray-inf2","docId":"ai/inference/Neuron/vllm-ray-inf2","unlisted":false},{"type":"link","label":"Mistral-7B on Inferentia2","href":"/ai-on-eks/docs/ai/inference/Neuron/Mistral-7b-inf2","docId":"ai/inference/Neuron/Mistral-7b-inf2","unlisted":false},{"type":"link","label":"Llama-3-8B on Inferentia2","href":"/ai-on-eks/docs/ai/inference/Neuron/llama3-inf2","docId":"ai/inference/Neuron/llama3-inf2","unlisted":false},{"type":"link","label":"Llama-2 on Inferentia2","href":"/ai-on-eks/docs/ai/inference/Neuron/llama2-inf2","docId":"ai/inference/Neuron/llama2-inf2","unlisted":false},{"type":"link","label":"Stable Diffusion on Inferentia2","href":"/ai-on-eks/docs/ai/inference/Neuron/stablediffusion-inf2","docId":"ai/inference/Neuron/stablediffusion-inf2","unlisted":false},{"type":"link","label":"Ray Serve High Availability","href":"/ai-on-eks/docs/ai/inference/Neuron/rayserve-ha","docId":"ai/inference/Neuron/rayserve-ha","unlisted":false}]}],"href":"/ai-on-eks/docs/category/inference-on-eks"}],"blueprints":[{"type":"category","label":"AI/ML on EKS","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction","href":"/ai-on-eks/docs/blueprints/ai-ml/","docId":"blueprints/ai-ml/index","unlisted":false},{"type":"link","label":"EMR NVIDIA Spark-RAPIDS","href":"/ai-on-eks/docs/blueprints/ai-ml/emr-spark-rapids","docId":"blueprints/ai-ml/emr-spark-rapids","unlisted":false},{"type":"link","label":"JARK on EKS","href":"/ai-on-eks/docs/blueprints/ai-ml/jark","docId":"blueprints/ai-ml/jark","unlisted":false},{"type":"link","label":"JupyterHub on EKS","href":"/ai-on-eks/docs/blueprints/ai-ml/jupyterhub","docId":"blueprints/ai-ml/jupyterhub","unlisted":false},{"type":"link","label":"Trainium on EKS","href":"/ai-on-eks/docs/blueprints/ai-ml/trainium","docId":"blueprints/ai-ml/trainium","unlisted":false}],"href":"/ai-on-eks/docs/category/aiml-on-eks"},{"type":"link","label":"Troubleshooting","href":"/ai-on-eks/docs/blueprints/troubleshooting/","docId":"blueprints/troubleshooting/troubleshooting","unlisted":false}],"bestpractices":[{"type":"link","label":"Introduction","href":"/ai-on-eks/docs/bestpractices/intro","docId":"bestpractices/intro","unlisted":false},{"type":"link","label":"EKS Best Practices","href":"/ai-on-eks/docs/bestpractices/eks-best-practices/","docId":"bestpractices/eks-best-practices/eks-best-practices","unlisted":false},{"type":"link","label":"Preload container images into data volumes","href":"/ai-on-eks/docs/bestpractices/preload-container-images","docId":"bestpractices/preload-container-images","unlisted":false},{"type":"link","label":"Networking for AI","href":"/ai-on-eks/docs/bestpractices/networking/","docId":"bestpractices/networking/networking","unlisted":false}],"benchmarks":[],"resources":[{"type":"link","label":"Introduction","href":"/ai-on-eks/docs/resources/intro","docId":"resources/intro","unlisted":false},{"type":"link","label":"Mounpoint-S3 on EKS","href":"/ai-on-eks/docs/resources/mountpoint-s3","docId":"resources/mountpoint-s3","unlisted":false},{"type":"link","label":"Bin packing for Amazon EKS","href":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks","docId":"resources/binpacking-custom-scheduler-eks","unlisted":false}]},"docs":{"ai/index":{"id":"ai/index","title":"AI on EKS","description":"Welcome to AI on Amazon Elastic Kubernetes Service (EKS), your gateway to harnessing the power of Large Language Models (LLMs) for a wide range of applications. This introduction page serves as your starting point to explore our architectural patterns and blueprints for Training, Fine-tuning, and Inference using the latest LLMs.","sidebar":"ai"},"ai/inference/GPUs/nvidia-nim-llama3":{"id":"ai/inference/GPUs/nvidia-nim-llama3","title":"NVIDIA NIM LLM on Amazon EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"ai"},"ai/inference/GPUs/nvidia-nim-operator":{"id":"ai/inference/GPUs/nvidia-nim-operator","title":"NVIDIA NIM Operator on EKS","description":"What is NVIDIA NIM?","sidebar":"ai"},"ai/inference/GPUs/ray-vllm-deepseek":{"id":"ai/inference/GPUs/ray-vllm-deepseek","title":"DeepSeek-R1 on EKS","description":"In this guide, we\'ll explore deploying DeepSeek-R1-Distill-Llama-8B model inference using Ray with a vLLM backend on Amazon EKS.","sidebar":"ai"},"ai/inference/GPUs/stablediffusion-gpus":{"id":"ai/inference/GPUs/stablediffusion-gpus","title":"Stable Diffusion on GPU","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"ai"},"ai/inference/GPUs/vLLM-NVIDIATritonServer":{"id":"ai/inference/GPUs/vLLM-NVIDIATritonServer","title":"NVIDIA Triton Server with vLLM","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"ai"},"ai/inference/GPUs/vLLM-rayserve":{"id":"ai/inference/GPUs/vLLM-rayserve","title":"RayServe with vLLM","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"ai"},"ai/inference/Neuron/llama2-inf2":{"id":"ai/inference/Neuron/llama2-inf2","title":"Llama-2 on Inferentia2","description":"Serve Llama-2 models on AWS Inferentia accelerators for efficient inference.","sidebar":"ai"},"ai/inference/Neuron/llama3-inf2":{"id":"ai/inference/Neuron/llama3-inf2","title":"Llama-3-8B on Inferentia2","description":"Serve Llama-3 models on AWS Inferentia accelerators for efficient inference.","sidebar":"ai"},"ai/inference/Neuron/Mistral-7b-inf2":{"id":"ai/inference/Neuron/Mistral-7b-inf2","title":"Mistral-7B on Inferentia2","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"ai"},"ai/inference/Neuron/rayserve-ha":{"id":"ai/inference/Neuron/rayserve-ha","title":"Ray Serve High Availability","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"ai"},"ai/inference/Neuron/stablediffusion-inf2":{"id":"ai/inference/Neuron/stablediffusion-inf2","title":"Stable Diffusion on Inferentia2","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"ai"},"ai/inference/Neuron/vllm-ray-inf2":{"id":"ai/inference/Neuron/vllm-ray-inf2","title":"Llama-3-8B with vLLM on Inferentia2","description":"Serving Meta-Llama-3-8B-Instruct model on AWS Inferentia2 using Ray and vLLM for optimized inference performance.","sidebar":"ai"},"ai/infrastructure/infrastructure":{"id":"ai/infrastructure/infrastructure","title":"Introduction","description":"The AIoEKS foundational infrastructure lives in the infra/base directory. This directory contains the base","sidebar":"ai"},"ai/training/GPUs/bionemo":{"id":"ai/training/GPUs/bionemo","title":"BioNeMo on EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"ai"},"ai/training/Neuron/BERT-Large":{"id":"ai/training/Neuron/BERT-Large","title":"BERT-Large on Trainium","description":"COMING SOON","sidebar":"ai"},"ai/training/Neuron/Llama2":{"id":"ai/training/Neuron/Llama2","title":"Llama-2 with Nemo-Megatron on Trn1","description":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","sidebar":"ai"},"ai/training/Neuron/RayTrain-Llama2":{"id":"ai/training/Neuron/RayTrain-Llama2","title":"RayTrain-Llama2","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"ai"},"bestpractices/eks-best-practices/eks-best-practices":{"id":"bestpractices/eks-best-practices/eks-best-practices","title":"EKS Best Practices","description":"EKS Best Practices Guides","sidebar":"bestpractices"},"bestpractices/intro":{"id":"bestpractices/intro","title":"Introduction","description":"Through working with AWS customers, we\u2019ve identified a number of Best Practices that we have recommended for Spark or other large data workloads. We continue to collect and post those recommendations here. Because this is an ongoing effort, please open an Issue or Pull Request if you find something outdated so we can update it.","sidebar":"bestpractices"},"bestpractices/networking/networking":{"id":"bestpractices/networking/networking","title":"Networking for AI","description":"VPC and IP Considerations","sidebar":"bestpractices"},"bestpractices/preload-container-images":{"id":"bestpractices/preload-container-images","title":"Preload container images into data volumes","description":"The purpose of this pattern is to reduce the cold start time of containers with large images by caching the images in the data volume of Bottlerocket OS.","sidebar":"bestpractices"},"blueprints/ai-ml/emr-spark-rapids":{"id":"blueprints/ai-ml/emr-spark-rapids","title":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","description":"The NVIDIA RAPIDS Accelerator for Apache Spark is a powerful tool that builds on the capabilities of NVIDIA CUDA\xae - a transformative parallel computing platform designed for enhancing computational processes on NVIDIA\'s GPU architecture. RAPIDS, a project developed by NVIDIA, comprises a suite of open-source libraries that are hinged upon CUDA, thereby enabling GPU-accelerated data science workflows.","sidebar":"blueprints"},"blueprints/ai-ml/index":{"id":"blueprints/ai-ml/index","title":"AI/ML Platforms on Amazon EKS","description":"Amazon Elastic Kubernetes Service (EKS) is a powerful, managed Kubernetes platform that has become a cornerstone for deploying and managing AI/ML workloads in the cloud. With its ability to handle complex, resource-intensive tasks, Amazon EKS provides a scalable and flexible foundation for running AI/ML models, making it an ideal choice for organizations aiming to harness the full potential of machine learning.","sidebar":"blueprints"},"blueprints/ai-ml/jark":{"id":"blueprints/ai-ml/jark","title":"JARK on EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/ai-ml/jupyterhub":{"id":"blueprints/ai-ml/jupyterhub","title":"jupyterhub","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/ai-ml/trainium":{"id":"blueprints/ai-ml/trainium","title":"trainium","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/troubleshooting/troubleshooting":{"id":"blueprints/troubleshooting/troubleshooting","title":"Troubleshooting","description":"You will find troubleshooting info for AI on Amazon EKS(AIoEKS) installation issues","sidebar":"blueprints"},"intro":{"id":"intro","title":"intro","description":""},"resources/binpacking-custom-scheduler-eks":{"id":"resources/binpacking-custom-scheduler-eks","title":"Bin packing for Amazon EKS","description":"Introduction","sidebar":"resources"},"resources/intro":{"id":"resources/intro","title":"Introduction to Resources","description":"Welcome to the Resources section. This area is dedicated to providing you with a wealth of information, insights, and learning materials to enhance your understanding and skills in Data and AI workloads running on EKS.","sidebar":"resources"},"resources/mountpoint-s3":{"id":"resources/mountpoint-s3","title":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","description":"What is Mountpoint-S3?","sidebar":"resources"}}}}')}}]);